# üéØ PLANO DE OTIMIZA√á√ÉO DE M√âTRICAS - SANKOFA ENTERPRISE PRO

**Data**: 08 de Novembro de 2025  
**Objetivo**: Alcan√ßar m√©tricas ideais de detec√ß√£o de fraude  
**Meta**: F1-Score > 85%, Precision > 80%, Recall > 75%  

---

## üìä SITUA√á√ÉO ATUAL vs META

| M√©trica | Atual | Meta | Gap |
|---------|-------|------|-----|
| **Accuracy** | 48% | 85%+ | -37 pontos |
| **Precision** | 48% | 80%+ | -32 pontos |
| **Recall** | 100% | 75%+ | +25 pontos (precisa reduzir) |
| **F1-Score** | 64.88% | 85%+ | -20.12 pontos |
| **False Positive Rate** | 100% | <10% | -90 pontos |

---

## üîç DIAGN√ìSTICO DO PROBLEMA

### Problema Principal: Threshold Muito Baixo

O sistema est√° marcando **TODAS as transa√ß√µes como fraude** porque o **threshold de decis√£o est√° muito baixo** (provavelmente 0.3 ou menos).

```python
# C√≥digo atual (production_fraud_engine.py)
threshold_high_risk = 0.35
threshold_medium_risk = 0.2
detection_threshold = 0.3  # ‚Üê MUITO BAIXO!
```

**Consequ√™ncia**: Qualquer transa√ß√£o com probabilidade > 0.3 √© marcada como fraude, resultando em 100% de falsos positivos.

---

## üõ†Ô∏è SOLU√á√ïES PR√ÅTICAS

### Solu√ß√£o 1: Ajustar o Threshold de Decis√£o (CR√çTICO)

#### Passo 1: Encontrar o Threshold √ìtimo

**M√©todo**: Usar a curva ROC e Precision-Recall para encontrar o ponto √≥timo.

```python
# backend/ml_engine/threshold_optimizer.py
import numpy as np
from sklearn.metrics import precision_recall_curve, roc_curve, f1_score

class ThresholdOptimizer:
    """
    Otimiza o threshold de decis√£o para maximizar F1-Score.
    """
    
    def __init__(self, target_precision=0.80, target_recall=0.75):
        """
        Args:
            target_precision: Precision m√≠nima desejada (0.80 = 80%)
            target_recall: Recall m√≠nimo desejado (0.75 = 75%)
        """
        self.target_precision = target_precision
        self.target_recall = target_recall
    
    def find_optimal_threshold(
        self, 
        y_true: np.ndarray, 
        y_proba: np.ndarray
    ) -> dict:
        """
        Encontra o threshold √≥timo que maximiza F1-Score.
        
        Args:
            y_true: Labels verdadeiros (0 ou 1)
            y_proba: Probabilidades preditas (0.0 a 1.0)
        
        Returns:
            Dict com threshold √≥timo e m√©tricas
        """
        # Calcular precision e recall para diferentes thresholds
        precisions, recalls, thresholds = precision_recall_curve(y_true, y_proba)
        
        # Calcular F1-Score para cada threshold
        f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)
        
        # Encontrar threshold que maximiza F1-Score
        best_idx = np.argmax(f1_scores)
        best_threshold = thresholds[best_idx]
        best_f1 = f1_scores[best_idx]
        best_precision = precisions[best_idx]
        best_recall = recalls[best_idx]
        
        # Encontrar threshold que atende aos requisitos m√≠nimos
        valid_idx = np.where(
            (precisions >= self.target_precision) & 
            (recalls >= self.target_recall)
        )[0]
        
        if len(valid_idx) > 0:
            # Usar o threshold que maximiza F1 entre os v√°lidos
            valid_f1 = f1_scores[valid_idx]
            best_valid_idx = valid_idx[np.argmax(valid_f1)]
            recommended_threshold = thresholds[best_valid_idx]
            recommended_f1 = f1_scores[best_valid_idx]
            recommended_precision = precisions[best_valid_idx]
            recommended_recall = recalls[best_valid_idx]
        else:
            # Nenhum threshold atende aos requisitos, usar o melhor F1
            recommended_threshold = best_threshold
            recommended_f1 = best_f1
            recommended_precision = best_precision
            recommended_recall = best_recall
        
        return {
            'optimal_threshold': float(recommended_threshold),
            'f1_score': float(recommended_f1),
            'precision': float(recommended_precision),
            'recall': float(recommended_recall),
            'meets_requirements': (
                recommended_precision >= self.target_precision and 
                recommended_recall >= self.target_recall
            )
        }
    
    def plot_threshold_analysis(
        self, 
        y_true: np.ndarray, 
        y_proba: np.ndarray,
        save_path: str = None
    ):
        """
        Plota an√°lise de threshold (Precision-Recall e ROC).
        """
        import matplotlib.pyplot as plt
        
        # Precision-Recall curve
        precisions, recalls, thresholds = precision_recall_curve(y_true, y_proba)
        f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)
        
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        # Plot 1: Precision, Recall, F1 vs Threshold
        axes[0].plot(thresholds, precisions[:-1], label='Precision', linewidth=2)
        axes[0].plot(thresholds, recalls[:-1], label='Recall', linewidth=2)
        axes[0].plot(thresholds, f1_scores[:-1], label='F1-Score', linewidth=2, linestyle='--')
        axes[0].axhline(y=self.target_precision, color='r', linestyle=':', label=f'Target Precision ({self.target_precision})')
        axes[0].axhline(y=self.target_recall, color='g', linestyle=':', label=f'Target Recall ({self.target_recall})')
        axes[0].set_xlabel('Threshold', fontsize=12)
        axes[0].set_ylabel('Score', fontsize=12)
        axes[0].set_title('M√©tricas vs Threshold', fontsize=14, fontweight='bold')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # Plot 2: Precision-Recall curve
        axes[1].plot(recalls, precisions, linewidth=2)
        axes[1].set_xlabel('Recall', fontsize=12)
        axes[1].set_ylabel('Precision', fontsize=12)
        axes[1].set_title('Curva Precision-Recall', fontsize=14, fontweight='bold')
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"Gr√°fico salvo em: {save_path}")
        else:
            plt.show()
```

#### Passo 2: Executar Otimiza√ß√£o

```python
# backend/scripts/optimize_threshold.py
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

import pandas as pd
import numpy as np
from ml_engine.production_fraud_engine import get_fraud_engine
from ml_engine.threshold_optimizer import ThresholdOptimizer

# Carregar dados de valida√ß√£o
df_val = pd.read_csv('data/validation_data.csv')
X_val = df_val.drop('isFraud', axis=1)
y_val = df_val['isFraud']

# Carregar engine e fazer predi√ß√µes
engine = get_fraud_engine()
predictions = engine.predict(X_val)
y_proba = np.array([p.fraud_probability for p in predictions])

# Otimizar threshold
optimizer = ThresholdOptimizer(target_precision=0.80, target_recall=0.75)
result = optimizer.find_optimal_threshold(y_val.values, y_proba)

print("=" * 80)
print("OTIMIZA√á√ÉO DE THRESHOLD")
print("=" * 80)
print(f"Threshold √ìtimo: {result['optimal_threshold']:.4f}")
print(f"F1-Score: {result['f1_score']:.4f}")
print(f"Precision: {result['precision']:.4f}")
print(f"Recall: {result['recall']:.4f}")
print(f"Atende Requisitos: {result['meets_requirements']}")
print("=" * 80)

# Salvar gr√°fico
optimizer.plot_threshold_analysis(
    y_val.values, 
    y_proba, 
    save_path='reports/threshold_analysis.png'
)

# Atualizar configura√ß√£o
with open('backend/config/optimal_threshold.txt', 'w') as f:
    f.write(str(result['optimal_threshold']))
```

#### Passo 3: Atualizar o Motor com o Threshold √ìtimo

```python
# backend/ml_engine/production_fraud_engine.py (ATUALIZAR)

class ProductionFraudEngine:
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        # ... c√≥digo existente ...
        
        # Carregar threshold otimizado
        threshold_path = Path(__file__).parent.parent / 'config' / 'optimal_threshold.txt'
        if threshold_path.exists():
            with open(threshold_path, 'r') as f:
                self.confidence_threshold = float(f.read().strip())
            logger.info(f"Loaded optimized threshold: {self.confidence_threshold}")
        else:
            # Usar threshold padr√£o mais conservador
            self.confidence_threshold = 0.65  # ‚Üê AUMENTADO de 0.3 para 0.65
            logger.warning(f"Using default threshold: {self.confidence_threshold}")
```

---

### Solu√ß√£o 2: Melhorar a Engenharia de Features

#### Problema: Features Fracas

O modelo atual pode n√£o ter features discriminativas suficientes.

#### Novas Features a Adicionar

```python
# backend/ml_engine/feature_engineering.py

class AdvancedFeatureEngineering:
    """
    Engenharia de features avan√ßada para detec√ß√£o de fraude.
    """
    
    def create_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Cria features avan√ßadas."""
        df = df.copy()
        
        # 1. Features Temporais
        df['hour'] = pd.to_datetime(df['timestamp']).dt.hour
        df['day_of_week'] = pd.to_datetime(df['timestamp']).dt.dayofweek
        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
        df['is_night'] = df['hour'].between(22, 6).astype(int)
        df['is_business_hours'] = df['hour'].between(9, 18).astype(int)
        
        # 2. Features de Valor
        df['log_value'] = np.log1p(df['value'])
        df['value_rounded'] = (df['value'] % 1 == 0).astype(int)  # Valores redondos s√£o suspeitos
        
        # 3. Features de Comportamento do Cliente
        # Calcular estat√≠sticas hist√≥ricas por CPF
        client_stats = df.groupby('client_cpf').agg({
            'value': ['mean', 'std', 'count'],
            'transaction_type': lambda x: x.mode()[0] if len(x) > 0 else 'UNKNOWN'
        }).reset_index()
        
        client_stats.columns = ['client_cpf', 'avg_value', 'std_value', 'num_transactions', 'preferred_type']
        df = df.merge(client_stats, on='client_cpf', how='left')
        
        # Desvio do comportamento normal
        df['value_deviation'] = (df['value'] - df['avg_value']) / (df['std_value'] + 1e-10)
        df['is_new_client'] = (df['num_transactions'] < 5).astype(int)
        
        # 4. Features de Dispositivo
        device_stats = df.groupby('device_id').agg({
            'client_cpf': 'nunique',  # Quantos clientes usam este dispositivo
            'value': 'sum'
        }).reset_index()
        
        device_stats.columns = ['device_id', 'num_clients_per_device', 'total_value_device']
        df = df.merge(device_stats, on='device_id', how='left')
        
        # Dispositivo compartilhado √© suspeito
        df['is_shared_device'] = (df['num_clients_per_device'] > 1).astype(int)
        
        # 5. Features de Localiza√ß√£o
        df['is_high_risk_state'] = df['state'].isin(['SP', 'RJ']).astype(int)
        
        # 6. Features de Canal
        df['is_mobile'] = (df['channel'] == 'MOBILE').astype(int)
        df['is_pix'] = (df['transaction_type'] == 'PIX').astype(int)
        
        # 7. Features de Velocidade
        # Ordenar por cliente e timestamp
        df = df.sort_values(['client_cpf', 'timestamp'])
        df['time_since_last_transaction'] = (
            df.groupby('client_cpf')['timestamp']
            .diff()
            .dt.total_seconds()
            .fillna(999999)
        )
        
        # Transa√ß√µes muito r√°pidas s√£o suspeitas
        df['is_rapid_transaction'] = (df['time_since_last_transaction'] < 60).astype(int)
        
        return df
```

---

### Solu√ß√£o 3: Balancear o Dataset de Treinamento

#### Problema: Dataset Desbalanceado

Se o dataset tem muito mais transa√ß√µes leg√≠timas que fraudes, o modelo pode ter dificuldade em aprender.

#### T√©cnicas de Balanceamento

```python
# backend/ml_engine/data_balancing.py

from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.combine import SMOTETomek

class DataBalancer:
    """
    Balanceia dataset de fraude usando t√©cnicas de resampling.
    """
    
    def __init__(self, method='smote'):
        """
        Args:
            method: 'smote', 'undersample', 'smotetomek'
        """
        self.method = method
    
    def balance(self, X, y):
        """
        Balanceia o dataset.
        
        Args:
            X: Features
            y: Labels (0=leg√≠timo, 1=fraude)
        
        Returns:
            X_balanced, y_balanced
        """
        print(f"Dataset original: {len(y)} samples")
        print(f"  - Leg√≠timas: {(y==0).sum()} ({(y==0).sum()/len(y)*100:.1f}%)")
        print(f"  - Fraudes: {(y==1).sum()} ({(y==1).sum()/len(y)*100:.1f}%)")
        
        if self.method == 'smote':
            # SMOTE: Synthetic Minority Over-sampling Technique
            sampler = SMOTE(random_state=42, k_neighbors=5)
        elif self.method == 'undersample':
            # Random Under-sampling
            sampler = RandomUnderSampler(random_state=42)
        elif self.method == 'smotetomek':
            # SMOTE + Tomek Links (remove amostras amb√≠guas)
            sampler = SMOTETomek(random_state=42)
        else:
            raise ValueError(f"M√©todo inv√°lido: {self.method}")
        
        X_balanced, y_balanced = sampler.fit_resample(X, y)
        
        print(f"\nDataset balanceado: {len(y_balanced)} samples")
        print(f"  - Leg√≠timas: {(y_balanced==0).sum()} ({(y_balanced==0).sum()/len(y_balanced)*100:.1f}%)")
        print(f"  - Fraudes: {(y_balanced==1).sum()} ({(y_balanced==1).sum()/len(y_balanced)*100:.1f}%)")
        
        return X_balanced, y_balanced
```

---

### Solu√ß√£o 4: Ajustar Pesos das Classes

#### Alternativa ao Balanceamento

Se n√£o quiser modificar o dataset, ajuste os pesos das classes no modelo.

```python
# backend/ml_engine/production_fraud_engine.py (ATUALIZAR)

from sklearn.utils.class_weight import compute_class_weight

class ProductionFraudEngine:
    def train(self, X_train, y_train):
        """Treina o modelo com pesos de classe ajustados."""
        
        # Calcular pesos das classes
        classes = np.unique(y_train)
        class_weights = compute_class_weight(
            'balanced',
            classes=classes,
            y=y_train
        )
        class_weight_dict = {classes[i]: class_weights[i] for i in range(len(classes))}
        
        print(f"Class weights: {class_weight_dict}")
        
        # Treinar modelos com pesos
        self.rf_model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            class_weight=class_weight_dict,  # ‚Üê ADICIONAR
            random_state=42
        )
        
        self.gb_model = GradientBoostingClassifier(
            n_estimators=100,
            max_depth=5,
            learning_rate=0.1,
            random_state=42
        )
        
        # ... resto do c√≥digo ...
```

---

### Solu√ß√£o 5: Usar Ensemble com Vota√ß√£o Ponderada

#### Problema: Ensemble N√£o Otimizado

O ensemble atual pode estar dando peso igual a todos os modelos.

#### Solu√ß√£o: Vota√ß√£o Ponderada

```python
# backend/ml_engine/production_fraud_engine.py (ATUALIZAR)

from sklearn.ensemble import VotingClassifier

class ProductionFraudEngine:
    def train(self, X_train, y_train):
        """Treina ensemble com vota√ß√£o ponderada."""
        
        # Treinar modelos individuais
        rf = RandomForestClassifier(n_estimators=100, random_state=42)
        gb = GradientBoostingClassifier(n_estimators=100, random_state=42)
        lr = LogisticRegression(max_iter=1000, random_state=42)
        
        # Avaliar performance individual em valida√ß√£o
        X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(
            X_train, y_train, test_size=0.2, random_state=42
        )
        
        rf.fit(X_train_split, y_train_split)
        gb.fit(X_train_split, y_train_split)
        lr.fit(X_train_split, y_train_split)
        
        # Calcular F1-Score de cada modelo
        rf_f1 = f1_score(y_val_split, rf.predict(X_val_split))
        gb_f1 = f1_score(y_val_split, gb.predict(X_val_split))
        lr_f1 = f1_score(y_val_split, lr.predict(X_val_split))
        
        print(f"RF F1: {rf_f1:.4f}")
        print(f"GB F1: {gb_f1:.4f}")
        print(f"LR F1: {lr_f1:.4f}")
        
        # Normalizar pesos
        total_f1 = rf_f1 + gb_f1 + lr_f1
        rf_weight = rf_f1 / total_f1
        gb_weight = gb_f1 / total_f1
        lr_weight = lr_f1 / total_f1
        
        print(f"Pesos: RF={rf_weight:.3f}, GB={gb_weight:.3f}, LR={lr_weight:.3f}")
        
        # Criar ensemble com vota√ß√£o ponderada
        self.ensemble = VotingClassifier(
            estimators=[
                ('rf', rf),
                ('gb', gb),
                ('lr', lr)
            ],
            voting='soft',  # Usar probabilidades
            weights=[rf_weight, gb_weight, lr_weight]  # ‚Üê PESOS OTIMIZADOS
        )
        
        # Treinar ensemble no dataset completo
        self.ensemble.fit(X_train, y_train)
```

---

## üìã ROADMAP DE IMPLEMENTA√á√ÉO

### Semana 1: Ajuste de Threshold (PRIORIDADE M√ÅXIMA)
- [ ] Implementar `ThresholdOptimizer`
- [ ] Executar otimiza√ß√£o em dados de valida√ß√£o
- [ ] Atualizar `production_fraud_engine.py` com threshold √≥timo
- [ ] Validar m√©tricas ap√≥s ajuste

**Meta**: F1-Score > 75%

### Semana 2: Engenharia de Features
- [ ] Implementar `AdvancedFeatureEngineering`
- [ ] Adicionar 15+ novas features
- [ ] Re-treinar modelo com novas features
- [ ] Validar impacto nas m√©tricas

**Meta**: F1-Score > 80%

### Semana 3: Balanceamento e Pesos
- [ ] Implementar `DataBalancer`
- [ ] Testar SMOTE, undersample e SMOTETomek
- [ ] Ajustar pesos das classes
- [ ] Escolher melhor abordagem

**Meta**: F1-Score > 85%

### Semana 4: Otimiza√ß√£o de Ensemble
- [ ] Implementar vota√ß√£o ponderada
- [ ] Testar diferentes combina√ß√µes de modelos
- [ ] Calibrar probabilidades
- [ ] Valida√ß√£o final

**Meta**: F1-Score > 90%

---

## ‚úÖ CHECKLIST DE VALIDA√á√ÉO

- [ ] Threshold otimizado e documentado
- [ ] Features avan√ßadas implementadas
- [ ] Dataset balanceado ou pesos ajustados
- [ ] Ensemble otimizado com vota√ß√£o ponderada
- [ ] M√©tricas validadas em dados de teste separados
- [ ] F1-Score > 85%
- [ ] Precision > 80%
- [ ] Recall > 75%
- [ ] False Positive Rate < 10%
- [ ] Documenta√ß√£o atualizada

---

**Documento preparado por**: An√°lise Automatizada  
**Data**: 08 de Novembro de 2025  
**Vers√£o**: 1.0  
